# -*- coding: utf-8 -*-
"""phase2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B--LjV23b0TYiKTY-rXPbkqB6SL8xxef
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.feature_selection import SelectKBest,f_classif
from sklearn.feature_selection import SelectKBest , f_classif
from scipy.stats import zscore
from scipy.stats import chi2_contingency
from sklearn.feature_selection import SelectKBest, chi2 ,RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LassoCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("dataset.csv")

"""# Raw dataset"""

df.info()

print(df.head())

df1=pd.DataFrame(df)
print(df1)

"""# Statistical summaries

removing first column "PetID" from dataframe to maintain clarity and relevance in our visual representations , feature selection, model building..etc , making it more efficient for analysis and modeling.
"""

x=list(df.keys())[1:]
df2=df[x]

"""Central tendency measurements"""

df2.describe()

"""**Variance**"""

var_data = df ["AgeMonths"].var ()
print ("AgeMonths veriance = ",var_data)

var_data = df ["WeightKg"].var ()
print ("WeightKg veriance = ",var_data)

var_data = df ["Vaccinated"].var ()
print ("Vaccinatedvar veriancer =",  var_data)

var_data = df ["HealthCondition"].var ()
print ("HealthCondition veriance= " ,var_data)

var_data = df ["TimeInShelterDays"].var ()
print ("TimeInShelterDays veriance=",var_data)

var_data = df ["AdoptionFee"].var ()
print ("AdoptionFee veriance = ",var_data)

var_data = df ["PreviousOwner"].var ()
print ("PreviousOwner veriance = ",var_data)

var_data = df ["AdoptionLikelihood"].var ()
print ("AdoptionLikelihood veriance = ",var_data)

"""# Graphs and tables
To effectively visualize and understand the distribution of variables in our dataset, we used a combination of graphs and tables.

**1-Histogram of AgeMonths:**

The histogram displays the ages of pets available for adoption. A large number of pets in a certain age range indicates that age group is more prevalent in the shelter. We observed that most values fall approximately between 140 and 175 months.
"""

#Histogram of AgeMonths
plt.figure(figsize=(10,6))
sns.histplot(df1['AgeMonths'],bins=5,edgecolor='black',color='lavender')
plt.title('Distribution of pet age (months)')
plt.xlabel('AgeMonths')
plt.ylabel('Frequence')
plt.show()

"""**2-Bar plot for Adoption Likelihood (classlabel)**

Our bar plot displays the distribution of adoption likelihood among the pets in our dataset. This chart categorizes pets into two groups: those likely to be adopted, represented as "1," and those unlikely to be adopted, represented as "0.".the results reveal a significant disparity between these two categories. A higher count in the "unlikely Adopted"
"""

# Bar plot for Adoption Likelihood
adoption_counts = df1['AdoptionLikelihood'].value_counts()
plt.figure(figsize=(8, 5))
sns.barplot(x=adoption_counts.index, y=adoption_counts.values)
plt.title('Adoption Likelihood Distribution')
plt.xlabel('Adoption Likelihood (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])
plt.show()

"""**3-pie chart of random sample petType**

visually represents the frequency distribution of different pet types within the random sample of 50 rows, by highlighting which pet types are more common. it shows that the bird and rabbit are less represented in the sample, while cat and dog  are the most common pet type in this dataset sample.
"""

# Take a random sample of 50 rows
data_sample = df.sample(n=50, random_state=1)

# Calculate the frequency of each Pet Type in the sample
PetType_frequency = data_sample['PetType'].value_counts(normalize=True) * 100

# Plot the pie chart
plt.figure(figsize=(6, 6))
PetType_frequency.plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('The Pet Adoption Dataset Frequency for 50 Sample Data')
plt.ylabel('')
plt.show()

"""**4-Bar plot for petType**

A bar plot allows us to quickly see which size category has the most pets and which has the least. We notice that the ‘Medium’ bar is significantly longer than the others, indicating that there are more medium pets in the dataset compared to small or large pets.
"""

# Bar chart
sns.countplot(x='Size', data=df)
plt.title('Distribution of The size')
plt.show()

"""**5-Scatter plot**

 displays individual pets, with each point representing a pet plotted based on its weight and age. Different colors are used to distinguish pet types, allowing for easy comparison of how various species (e.g., dogs, cats, birds) are distributed by weight and age. By analyzing the plot, you may identify trends, such as whether certain types of pets tend to be heavier or lighter at specific ages, providing useful insights into pet characteristics within the dataset.
"""

# SCATTER PLOT:
#pet_sample = df1.sample(n=50 , random_state=1 )
plt.figure(figsize=(8,6))
sns.scatterplot(x= 'WeightKg' , y='AgeMonths' , data = df  , hue='PetType' , palette='Set2' )
plt.title('Scatter plot of WeightKg VS. pet Age Months ')
plt.xlabel('Pet WeightKg')
plt.ylabel('Age Months')
plt.show()

"""# Data Cleaning

**1-Missing values:**

refer to data points that are absent from the dataset
"""

# MISSING VALUE
print ("Missing value")
print(df1.isna().sum())

"""**2-duplicate rows:**

occur when the same record appears multiple times in the dataset, it may occur because errors in data collection or merging processes.
"""

# duplicate Rows
duplicates = df1.duplicated()
print("Duplicate Rows (True indicates a duplicate):")
print(duplicates)

"""**3-Outliers**

refer to the data objects that doesn't comply with the behaviour or pattern  of the rest of the objects
"""

# Calculate Z-scores for a AgeMonths column
age_array = df1['AgeMonths'].to_numpy()
z_score = zscore(age_array)
threshold = 2

outliers = [age_array[i] for i, z in enumerate(z_score) if abs(z) > threshold]
print("Outliers using Z-Score in AgeMonths column:")
print(outliers)

# Calculate Z-scores for a AdoptionFee column
AdoptionFee_array = df1['AdoptionFee'].to_numpy()
z_score = zscore(AdoptionFee_array)
threshold = 2

outliers = [AdoptionFee_array[i] for i, z in enumerate(z_score) if abs(z) > threshold]
print("Outliers using Z-Score in AdoptionFee column:")
print(outliers)

# Calculate Z-scores for a TimeInShelterDays column
TimeInShelterDays_array = df1['TimeInShelterDays'].to_numpy()
z_score = zscore(TimeInShelterDays_array)
threshold = 2

outliers = [TimeInShelterDays_array[i] for i, z in enumerate(z_score) if abs(z) > threshold]
print("Outliers using Z-Score in TimeInShelterDays column:")
print(outliers)

# Calculate Z-scores for a WeightKg column
WeightKg_array = df1['WeightKg'].to_numpy()
z_score = zscore(WeightKg_array)
threshold = 2

outliers = [WeightKg_array[i] for i, z in enumerate(z_score) if abs(z) > threshold]
print("Outliers using Z-Score in WeightKg column:")
print(outliers)

"""# Conclusion

In summary, since there is no missing values, outliers, or duplicates in our dataset, the cleanliness of our dataset enhances the quality of analysis. so we can start proceed to data preprocessing techniques.

# Correlation Analysis

using chi square (Nominal attribute)

the correlation matrix helps identify relationships between variables,  to make informed decisions about which features to select based on their correlation with the target variable.
"""

# Sample DataFrame
data = {
    'AgeMonths': [2, 8, 15, 30, 150],
    'TimeInShelterDays': [5, 15, 35, 70, 85],
    'AdoptionStatus': [1, 0, 1, 0, 1]  # Target variable (1 for adopted, 0 for not adopted)
}
df = pd.DataFrame(data)

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Display the correlation matrix
print("Correlation Matrix:\n", correlation_matrix)

# Set a threshold for the correlation
threshold = 0.3  # Example threshold

# Features correlated with the target variable
correlation_with_target = correlation_matrix['AdoptionStatus']
relevant_features = correlation_with_target[abs(correlation_with_target) > threshold]

# Display relevant features
print("\nRelevant Features based on correlation with AdoptionStatus:\n", relevant_features)

"""# Transformation (Preproccessing)

**1-Normalization:**

Scaling the data to a specific range, often [0, 1].
"""

# NORMALIZATION
column_to_normilaize=[ 'WeightKg','AdoptionFee' ]
data_to_normalize = df1[column_to_normilaize]

minmax_scaler = MinMaxScaler()
normalized_data_minmax = minmax_scaler.fit_transform(data_to_normalize)

df1[column_to_normilaize] = normalized_data_minmax

print("Min-Max scaled data")
print(df1)

"""By normalizing WeightKg and AdoptionFee, we ensure that  different features can be compared .

**2-Discretization**

converting continuous numerical values into discrete categories or bins.
"""

# DISCRETIZATION

# For AgeMonths
column_to_discretize = ['AgeMonths', 'TimeInShelterDays']
age_bins = [0, 6, 12, 24, 60, 120, 180]
age_labels = ['1-6 months', '7-12 months', '13-24 months', '25-60 months', '61-120 months', '121-180 months']
df1['discretized_AgeMonths'] = pd.cut(df1['AgeMonths'], bins=age_bins, labels=age_labels, right=True)

# For TimeInShelterDays
shelter_bins = [0, 10, 30, 60, 90]
shelter_labels = ['Very Short Stay', 'Short Stay', 'Medium Stay', 'Long Stay']
df1['discretized_TimeInShelterDays'] = pd.cut(df1['TimeInShelterDays'], bins=shelter_bins, labels=shelter_labels, right=False)

# Display the updated DataFrame
print(df1[['AgeMonths', 'discretized_AgeMonths']])
print(df1[['TimeInShelterDays', 'discretized_TimeInShelterDays']])

"""discretizing these variables helps to categorize pets into age groups and shelter duration ranges. This is beneficial for improving model performance by reducing the complexity of inputs, which can help algorithms find patterns more efficiently.

**3-Encoding**

transforms categorical variables into a numerical format
"""

#TRANSFORMATION(ENCODING)
from sklearn.preprocessing import LabelEncoder
import pandas as pd
from scipy import stats


le = LabelEncoder()
df1['PetType']=le.fit_transform(df1['PetType'])
df1['Size']=le.fit_transform(df1['Size'])
df1['Breed']=le.fit_transform(df1['Breed'])

print(df1)

"""**PetType**: types like "Dog," "Cat," etc. Encoding allows the model to understand these categories as distinct groups.

**Size**: Similarly, sizes like "Small," "Medium," and "Large" are converted into numerical values, enabling the model to analyze their impact on adoption likelihood.

**Breed**: With  many different breeds, encoding helps represent this information numerically.

# Feature selection

**1-Filter FS Method using univarince method**

How It Works: Evaluates features independently using statistical tests.

**Conclusion:**  filter method successfully identified four important features that significantly contribute to the model's predictive performance, achieving an accuracy of 83%. This method is good for initial feature selection but does not consider feature interactions.
"""

#using univarince model
X = df2.drop(columns=['AdoptionLikelihood'])
y = df2['AdoptionLikelihood']

# Encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
#70% of the data is used for training , 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply Univariate Feature Selection
selector = SelectKBest(score_func=chi2, k=4)  # Select top 4 features based on their relationship with the target variable, using the Chi-squared statistical test.
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)  # Transform the test set

# Get the selected feature names
selected_features = X.columns[selector.get_support()]

print("Selected Features:")
print(selected_features)

# Fit a model using the selected features
model = LogisticRegression()
model.fit(X_train_selected, y_train)

# Calculate accuracy
y_pred = model.predict(X_test_selected)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the model with selected features:", accuracy)

"""**2-Wrapper method ( recursive feature elimination)**

How It Works: Trains a model, removes least important features iteratively.

**Conclusion:** it select the same selected features as the filter method but achieved a higher accuracy of 89%. This indicates that RFE is effective in identifying the most relevant features while considering their interactions.
"""

X = df2.drop(columns=['AdoptionLikelihood'])
y = df2['AdoptionLikelihood']
X = pd.get_dummies(X, drop_first=True) #converted to a numerical format using one-hot encoding, which is necessary for the model.

# Split the data into training and testing sets
#70% of the data is used for training , 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42) #allows the model more iterations to find the best solution if needed. for random state its the most papular used


num_features_to_select = 4 # select 4 top feature , It fits the model to the training data and starts eliminating less important features.

# Initialize RFE for feature selection
selector = RFE(estimator=model, n_features_to_select=num_features_to_select)
selector.fit(X_train, y_train)

# Transform the data for selected features
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

# Train the model with selected features
model.fit(X_train_selected, y_train)

# Evaluate accuracy

y_pred = model.predict(X_test_selected)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print selected features and their ranking
print("Selected Features:", selected_features.tolist())

"""**3-Embedded FS Method L1 Regularization.**

How It Works: Combines feature selection with model training (e.g., Lasso).

**Conclusion:** The embedded method selected a slightly different set of features, including Vaccinated instead of AgeMonths. It achieved an accuracy of 86%, demonstrating that L1 regularization can effectively identify important features while maintaining strong predictive performance.
"""

# Convert categorical variables to numerical to helps the model understand the data
df2 = pd.get_dummies(df2, drop_first=True)

# Define features and target variable
x = df2.drop(columns=['AdoptionLikelihood'])
y = df2['AdoptionLikelihood']

# Split the data into training and testing sets
#70% of the data is used for training , 30% for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Standardize the features for equal weight
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Create and fit the Lasso model
model = Lasso(alpha=0.1)
model.fit(x_train_scaled, y_train)

# identifies which features were selected based on their coefficients
#Features with non-zero coefficients are considered important for the model.
selected_features = x.columns[model.coef_ != 0]
print("Selected Features:", selected_features)

# Make predictions to calculate the accuracy
y_pred = model.predict(x_test_scaled)
y_pred_binary = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_binary)
print("Accuracy with selected features:", accuracy)

"""# **Conclusion:**


all approaches identified relevant features that contribute to the predictive ability of the model.
Best Method: Wrapper Method (RFE) Reason for Choice:

1-Highest Accuracy: The Wrapper Method achieved the highest accuracy of 0.89, indicating its effectiveness in capturing feature interactions and that it made the best predictions among the three methods.

2-Selected Features Consistency: It selected features that were also chosen by the Embedded Method which shows the ability to perform feature selection during model training. (like HealthCondition, Breed_Labrador, and Size_Medium), showing that it is likely capturing important information effectively.

making it the preferred choice for this dataset.

and we can see that ['Vaccinated', 'HealthCondition', 'Breed_Labrador', 'Size_Medium'] provide clear insights into what influences pet adoption, helping guide strategies for increasing adoption rates.

# contingency_table
"""

#contingency_table

contingency_table = pd.crosstab(df['PetType'], df['Breed'])
print("Contingency Table:")
print(contingency_table)

# Perform the Chi-Square test
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
print("\nChi-Square Statistics:", chi2_stat)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:")
print(expected)

# Set alpha level
alpha = 0.05
print("\nAlpha Level:", alpha)

# Check if we reject the null hypothesis
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant association between PetType and Breed.")
else:
    print("Fail to reject the null hypothesis: There is no significant association between PetType and Breed.")

"""During our correlation analysis,given the significant Chi-Square statistic, we observed a significant relationship between the variables PetType and Breed.and since every pet have one breed only unless the Dog and the Cat breeds have the aproximistly the same number. We decided to select only one of these correlated variables to reduce redundancy in our dataset and simplify our model, we will select the more relevant variable for our analysis"PetType".

**Removing 'Breed' column**
"""

print("Before dropping 'Breed' column:")
print(df1.head())

# Drop the 'Breed' column
df1.drop(columns=['Breed'], inplace=True)

# Display the first few rows of the dataset after dropping the column
print("\nAfter dropping 'Breed' column:")
print(df1.head())